// avro consumer
docker exec -it schema-registry /usr/bin/kafka-avro-console-consumer --topic summed-rated-movies --bootstrap-server broker:9092 --from-beginning

// rating producer
docker exec -i schema-registry /usr/bin/kafka-avro-console-producer --topic ratings --broker-list broker:9092 --property value.schema="$(< src/main/avro/rating.avsc)"
// ratings
{"id": 294, "rating": 8.2}
{"id": 294, "rating": 8.5}
{"id": 354, "rating": 9.9}
{"id": 354, "rating": 9.7}
{"id": 782, "rating": 7.8}
{"id": 782, "rating": 7.7}
{"id": 128, "rating": 8.7}
{"id": 128, "rating": 8.4}
{"id": 780, "rating": 2.1}
{"id": 100, "rating": 6.3}
{"id": 120, "rating": 7.2}
{"id": 140, "rating": 4.5}
{"id": 120, "rating": 8.9}

// movie producer
docker exec -i schema-registry /usr/bin/kafka-avro-console-producer --topic movies --broker-list broker:9092 --property value.schema="$(< src/main/avro/movie.avsc)"
// movies
{"id": 294, "title": "Die Hard", "release_year": 1988}
{"id": 354, "title": "Tree of Life", "release_year": 2011}
{"id": 782, "title": "A Walk in the Clouds", "release_year": 1995}
{"id": 128, "title": "The Big Lebowski", "release_year": 1998}
{"id": 100, "title": "Spiderman", "release_year": 2002}
{"id": 120, "title": "Pirates of The Caribbean", "release_year": 2003}
{"id": 140, "title": "La Grande Bellezza", "release_year": 2013}

// scored-movie producer
docker exec -i schema-registry /usr/bin/kafka-avro-console-producer --topic mintopk-scored-rated-movies --broker-list broker:9092 --property value.schema="$(< src/main/avro/scored-movie.avsc)"
// scored-movies: movie.getAverage()/10 * 0.8 + movie.getReleaseYear()/2020 * 0.2
    see score-movies.txt
1)  implementare un semplice topK in locale

2)  calcolare i topk sulle singole partizioni
    unire i risultati in un topic senza partizione

3)  usare uno stato distribuito per estrapolare i topK

4)  implementare algoritmo: vedi se già c'è
    prima centralizzato senza partizioni
    poi con partizioni

score = rating/10 * 0.8 + release_year/2020 * 0.2

loggare tempi di esecuzione su ogni kstream

TASKS:
[x] implementazione centralizzata di materialize score & sort
[x] implementazione distribuita di materialize score & sort
    - [x] Ogni processo ordina e scrive su un topic i topK locali -> vado a prendere i valori da quel topic
    - [x] Special topic con key la posizione nella classica, inviare al topic solo i valori che cambiano posizione in classifica
          1) salvarsi in uno store l'attuale lista ordinata
          2) generare la nuova lista ordinata
          3) confrontare gli elementi delle liste, in caso di elementi diversi -> forward(key=position,value=ScoredMovie)
[ ] implementazione centralizzata di [1] non gestisce gli aggiornamenti (non arrivano 2 record uguali) assunzione che ogni record è diverso dall'altro
[ ] progettazione di implementazione distribuita di [1]
    - non è possibile modificare lo state store usando Interactive Queries --> non si può gestire una struttura dati
      distribuita con lo state Store
    - utilizzando uno state store distribuito per contare il numero di record si può usare l'algoritmo localmente
      tenendo conto del numero totale di record per gestire le windows --> topK locali per ogni finestra --> aggregazione
      e topK globale centralizzato per ogni window

[1] D. Yang, A. Shastri, E.A. Rundensteiner and M.O. Ward,
Anoptimal strategy for monitoring top-k queries in streaming windows,
in:Proceedings of the 14th International Conference onExtending Database Technology, ACM, 2011, pp. 57–68.


Eventi a frequenza costante: un evento ogni tot secondi.
Latenza: invio evento 9-13-17, fine evento risposta con i topk
Throughput: massimo input throughput

How to run on aws:
- copy jar file using `scp -i *.pem jar-file.jar ubuntu@DNS:~/kstream`
- connect using SSH `ssh -i *.pem ubuntu@DNS`

How to create kafka connector:
`curl -X POST \
      -H "Content-Type: application/json" \
      --data '{ "name": "mintopkn-jdbc-source", "config": {
        "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
        "tasks.max": 3,
        "connection.url": "jdbc:mysql://mysql:3306/mintopkn?user=luca&password=passwd",
        "mode": "timestamp+incrementing",
        "incrementing.column.name": "id",
        "timestamp.column.name": "modified",
        "topic.prefix": "mintopkn-jdbc-",
        "poll.interval.ms": 1000,
        "transforms" : "AddNamespace",
        "transforms.AddNamespace.type" : "org.apache.kafka.connect.transforms.SetSchemaMetadata$Value",
        "transforms.AddNamespace.schema.name" : "myapp.avro.MovieIncome"} }' \
      http://$CONNECT_HOST:28083/connectors`

Run DisMSSTOPK on AWS:
nohup java -jar RatingsDriver.jar pdmss-scored-rated-movies-dataset0 1 0 &
nohup java -jar PhysicalWindowDistributedMSS.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 150 4 0 &

nohup java -jar PhysicalWindowDistributedMSS.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 150 4 1 &

nohup java -jar PhysicalWindowDistributedMSS.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 150 4 2 &

nohup java -jar PhysicalWindowDistributedMSS.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 150 4 3 &

nohup java -jar PhysicalWindowDistributedMSS.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 150 4 4 &

nohup java -jar PhysicalWindowDistributedMSS.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 150 4 5 &

nohup java -jar PhysicalWindowCentralizedAggregatedSort.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 150 4 &

nohup java -jar PhysicalWindowDistributedMSS.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 5 0 6 &

nohup java -jar PhysicalWindowDistributedMSS.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 5 0 7 &

nohup java -jar PhysicalWindowDistributedMSS.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 5 0 8 &

nohup java -jar PhysicalWindowDistributedMSS.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 5 0 9 &

sudo mkfs.ext4 -E nodiscard /dev/nvme1n1
sudo mount /dev/nvme1n1 /vol1
sudo mkdir /vol1/docker-data
sudo service docker restart
sudo mkdir -p -m777 /vol1/rockdb
sudo mkdir -p /vol1/zk-data
sudo mkdir -p /vol1/zk-txn-logs
sudo mkdir -p /vol1/kafka-data

maybe need to delete content of /vol1/docker-data

min.score per ogni finestra
re-run Distributed MinTopK with topk=50 expecially for dataset2
re-run Distributed MSSTopK with topK=10 for dataset 3 & 4

Forse andrebbe rivista la misurazione della latency, prendere come tempo iniziale il primo record della finestra invece
dell'ultimo.
Forse si può misurare quanto tempo i 2 algoritmi impiegano a elaborare un tot numero di dati.
Vedo una differenza nell'ingestione dei dati nel caso centralizzato e in quello distribuito.

Ho provato finestre di dimensioni diverse (1200, 3600, 7200, 36000)
36000 sono troppo grandi forse
Ho provato diversi numeri di instanze (3,6,10)
Ho diminuito l'input throughput da 200 records/sec a 100 records/sec

Tempo totale esperimento
Latenza nuova: ultimo topK in output - primo record finestra

con SIZE=3600 il centralizzato è molto più lento rispetto a SIZE=1200 -> usare 3600
il tempo totale usando 5ms o 10ms come intervallo di invio è circa uguale nel caso di SIZE=3600 (vedi esperimenti con topK=10)
per il distribuito ho già le misurazioni usando 10ms per il dataset0 quindi userei 10ms per gli esperimenti.


---- CALL 9 ottobre
durata exp: 160m --> 9.600.000 ms
distanza tra eventi: 10 ms
numero di eventi: 250.000

tempo di invio: 250.000 * 10 ms = 2.500.000
latenza: 293.000 ms

408 finestre

nuovo setting

numero di eventi: 100.000
KPI: tempo di esecuzione

da cui possiamo calcolare il throughput
- 100.000/9.700 = 10 record/sec
- 408/161 = 2,4 finestre al minuto


spazio dell'esperimento

K: 2, 10, 50
window size: 1200, 3600
hopping size: 300
alg: m&s, minTopK
distribuzione: 1 (centralizzato), 3, 6

per sapere se serve fare 5 esperimenti proverei solo: {10} x {3600} x {300} x {m&s, minTopK} x {1,6}

nohup java -jar DistributedMinTopK.jar src/main/java/myapp/distributedMinTopK/disMinTopK.env 200 4 0 &

nohup java -jar DistributedMinTopK.jar src/main/java/myapp/distributedMinTopK/disMinTopK.env 200 4 1 &

nohup java -jar DistributedMinTopK.jar src/main/java/myapp/distributedMinTopK/disMinTopK.env 200 4 2 &

nohup java -jar DistributedMinTopK.jar src/main/java/myapp/distributedMinTopK/disMinTopK.env 200 4 3 &

nohup java -jar DistributedMinTopK.jar src/main/java/myapp/distributedMinTopK/disMinTopK.env 200 4 4 &

nohup java -jar DistributedMinTopK.jar src/main/java/myapp/distributedMinTopK/disMinTopK.env 200 4 5 &

nohup java -jar CentralizedTopK.jar src/main/java/myapp/distributedMinTopK/disMinTopK.env 200 4 DisMinTopK &

nohup java -jar DistributedMinTopK.jar src/main/java/myapp/distributedMinTopK/disMinTopK.env 10 4 6 &

nohup java -jar DistributedMinTopK.jar src/main/java/myapp/distributedMinTopK/disMinTopK.env 10 4 7 &

nohup java -jar DistributedMinTopK.jar src/main/java/myapp/distributedMinTopK/disMinTopK.env 10 4 8 &

nohup java -jar DistributedMinTopK.jar src/main/java/myapp/distributedMinTopK/disMinTopK.env 10 4 9 &


nohup java -jar out/artifacts/DistributedMinTopK_jar/DistributedMinTopK.jar src/main/java/myapp/distributedMinTopK/disMinTopK.env 10 0 0 &

nohup java -jar out/artifacts/DistributedMinTopK_jar/DistributedMinTopK.jar src/main/java/myapp/distributedMinTopK/disMinTopK.env 10 0 1 &

nohup java -jar out/artifacts/DistributedMinTopK_jar/DistributedMinTopK.jar src/main/java/myapp/distributedMinTopK/disMinTopK.env 10 0 2 &

nohup java -jar out/artifacts/CentralizedTopK_jar/CentralizedTopK.jar src/main/java/myapp/distributedMinTopK/disMinTopK.env 10 0 DisMinTopK &

Ho realizzato esperimenti:
- DisMSSTopK: topK=10 instances={3,6,10}; instances=6 topK={5,10,50,100,300}
- DisMinTopK: topK=10 instances={3,6,10}; instances=6 topK={5,10,50,100,300}

python3 pythonScripts/speedUpPlot.py measurements/DisMSSTopK/top5/dataset0/100Krecords_3600_300_5K_total_time_total_time_statistic.csv measurements/DisMSSTopK/top10/dataset0/100Krecords_3600_300_10K_total_time_statistic_6instances.csv measurements/DisMSSTopK/top50/dataset0/100Krecords_3600_300_50K_total_time_total_time_statistic.csv measurements/DisMSSTopK/top100/dataset0/100Krecords_3600_300_100K_total_time_total_time_statistic.csv measurements/DisMSSTopK/top150/dataset0/100Krecords_3600_300_150K_total_time_total_time_statistic.csv measurements/DisMSSTopK/top200/dataset0/100Krecords_3600_300_200K_total_time_total_time_statistic.csv measurements/DisMSSTopK/top300/dataset0/100Krecords_3600_300_300K_total_time_total_time_statistic.csv measurements/DisMinTopK/top5/dataset0/100Krecords_3600_300_5K_total_time_total_time_statistic.csv measurements/DisMinTopK/top10/dataset0/100Krecords_3600_300_10K_total_time_statistic_6instances.csv measurements/DisMinTopK/top50/dataset0/100Krecords_3600_300_50K_total_time_total_time_statistic.csv measurements/DisMinTopK/top100/dataset0/100Krecords_3600_300_100K_total_time_total_time_statistic.csv measurements/DisMinTopK/top150/dataset0/100Krecords_3600_300_150K_total_time_total_time_statistic.csv measurements/DisMinTopK/top200/dataset0/100Krecords_3600_300_200K_total_time_total_time_statistic.csv measurements/DisMinTopK/top300/dataset0/100Krecords_3600_300_300K_total_time_total_time_statistic.csvsurements/DisMinTopK/top10/dataset0/100Krecords_3600_300_10K_total_time_6instances.csv measurements/DisMinTopK/top50/dataset0/100Krecords_3600_300_50K_total_time_total_time_statistic.csv measurements/DisMinTopK/top100/dataset0/100Krecords_3600_300_100K_total_time_total_time_statistic.csv measurements/DisMinTopK/top150/dataset0/100Krecords_3600_300_150K_total_time_total_time_statistic.csv measurements/DisMinTopK/top200/dataset0/100Krecords_3600_300_200K_total_time_total_time_statistic.csv measurements/DisMinTopK/top300/dataset0/100Krecords_3600_300_300K_total_time_total_time_statistic.csv