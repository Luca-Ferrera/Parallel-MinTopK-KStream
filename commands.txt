// avro consumer
docker exec -it schema-registry /usr/bin/kafka-avro-console-consumer --topic summed-rated-movies --bootstrap-server broker:9092 --from-beginning

// rating producer
docker exec -i schema-registry /usr/bin/kafka-avro-console-producer --topic ratings --broker-list broker:9092 --property value.schema="$(< src/main/avro/rating.avsc)"
// ratings
{"id": 294, "rating": 8.2}
{"id": 294, "rating": 8.5}
{"id": 354, "rating": 9.9}
{"id": 354, "rating": 9.7}
{"id": 782, "rating": 7.8}
{"id": 782, "rating": 7.7}
{"id": 128, "rating": 8.7}
{"id": 128, "rating": 8.4}
{"id": 780, "rating": 2.1}
{"id": 100, "rating": 6.3}
{"id": 120, "rating": 7.2}
{"id": 140, "rating": 4.5}
{"id": 120, "rating": 8.9}

// movie producer
docker exec -i schema-registry /usr/bin/kafka-avro-console-producer --topic movies --broker-list broker:9092 --property value.schema="$(< src/main/avro/movie.avsc)"
// movies
{"id": 294, "title": "Die Hard", "release_year": 1988}
{"id": 354, "title": "Tree of Life", "release_year": 2011}
{"id": 782, "title": "A Walk in the Clouds", "release_year": 1995}
{"id": 128, "title": "The Big Lebowski", "release_year": 1998}
{"id": 100, "title": "Spiderman", "release_year": 2002}
{"id": 120, "title": "Pirates of The Caribbean", "release_year": 2003}
{"id": 140, "title": "La Grande Bellezza", "release_year": 2013}

// scored-movie producer
docker exec -i schema-registry /usr/bin/kafka-avro-console-producer --topic mintopk-scored-rated-movies --broker-list broker:9092 --property value.schema="$(< src/main/avro/scored-movie.avsc)"
// scored-movies: movie.getAverage()/10 * 0.8 + movie.getReleaseYear()/2020 * 0.2
    see score-movies.txt
1)  implementare un semplice topK in locale

2)  calcolare i topk sulle singole partizioni
    unire i risultati in un topic senza partizione

3)  usare uno stato distribuito per estrapolare i topK

4)  implementare algoritmo: vedi se già c'è
    prima centralizzato senza partizioni
    poi con partizioni

score = rating/10 * 0.8 + release_year/2020 * 0.2

loggare tempi di esecuzione su ogni kstream

TASKS:
[x] implementazione centralizzata di materialize score & sort
[x] implementazione distribuita di materialize score & sort
    - [x] Ogni processo ordina e scrive su un topic i topK locali -> vado a prendere i valori da quel topic
    - [x] Special topic con key la posizione nella classica, inviare al topic solo i valori che cambiano posizione in classifica
          1) salvarsi in uno store l'attuale lista ordinata
          2) generare la nuova lista ordinata
          3) confrontare gli elementi delle liste, in caso di elementi diversi -> forward(key=position,value=ScoredMovie)
[ ] implementazione centralizzata di [1] non gestisce gli aggiornamenti (non arrivano 2 record uguali) assunzione che ogni record è diverso dall'altro
[ ] progettazione di implementazione distribuita di [1]
    - non è possibile modificare lo state store usando Interactive Queries --> non si può gestire una struttura dati
      distribuita con lo state Store
    - utilizzando uno state store distribuito per contare il numero di record si può usare l'algoritmo localmente
      tenendo conto del numero totale di record per gestire le windows --> topK locali per ogni finestra --> aggregazione
      e topK globale centralizzato per ogni window

[1] D. Yang, A. Shastri, E.A. Rundensteiner and M.O. Ward,
Anoptimal strategy for monitoring top-k queries in streaming windows,
in:Proceedings of the 14th International Conference onExtending Database Technology, ACM, 2011, pp. 57–68.


Eventi a frequenza costante: un evento ogni tot secondi.
Latenza: invio evento 9-13-17, fine evento risposta con i topk
Throughput: massimo input throughput

How to run on aws:
- copy jar file using `scp -i *.pem jar-file.jar ubuntu@DNS:~/kstream`
- connect using SSH `ssh -i *.pem ubuntu@DNS`

How to create kafka connector:
`curl -X POST \
      -H "Content-Type: application/json" \
      --data '{ "name": "mintopkn-jdbc-source", "config": {
        "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
        "tasks.max": 3,
        "connection.url": "jdbc:mysql://mysql:3306/mintopkn?user=luca&password=passwd",
        "mode": "timestamp+incrementing",
        "incrementing.column.name": "id",
        "timestamp.column.name": "modified",
        "topic.prefix": "mintopkn-jdbc-",
        "poll.interval.ms": 1000,
        "transforms" : "AddNamespace",
        "transforms.AddNamespace.type" : "org.apache.kafka.connect.transforms.SetSchemaMetadata$Value",
        "transforms.AddNamespace.schema.name" : "myapp.avro.MovieIncome"} }' \
      http://$CONNECT_HOST:28083/connectors`

Run DisMSSTOPK on AWS:
nohup java -jar RatingsDriver.jar pdmss-scored-rated-movies 10 0 &
nohup java -jar PhysicalWindowCentralizedAggregatedSort.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 2 0 &
nohup java -jar PhysicalWindowDistributedMSS.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 2 0 0 &
nohup java -jar PhysicalWindowDistributedMSS.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 2 0 1 &
nohup java -jar PhysicalWindowDistributedMSS.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 2 0 2 &
nohup java -jar PhysicalWindowDistributedMSS.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 2 0 3 &
nohup java -jar PhysicalWindowDistributedMSS.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 2 0 4 &
nohup java -jar PhysicalWindowDistributedMSS.jar "src/main/java/myapp/distributedMaterializeScoreSort/PhysicalWindow/physicalWindowDisMSS.env" 2 0 5 &

sudo mkfs.ext4 -E nodiscard /dev/nvme1n1
sudo mount /dev/nvme1n1 /vol1
sudo mkdir /vol1/docker-data
sudo service docker restart
sudo mkdir /vol1/rockdb
sudo mkdir -p /vol1/zk-data
sudo mkdir -p /vol1/zk-txn-logs
sudo mkdir -p /vol1/kafka-data

maybe need to delete content of /vol1/docker-data

min.score per ogni finestra
re-run Distributed MinTopK with topk=50 expecially for dataset2
re-run Distributed MSSTopK with topK=10 for dataset 3 & 4

Forse andrebbe rivista la misurazione della latency, prendere come tempo iniziale il primo record della finestra invece
dell'ultimo.
Forse si può misurare quanto tempo i 2 algoritmi impiegano a elaborare un tot numero di dati.
Vedo una differenza nell'ingestione dei dati nel caso centralizzato e in quello distribuito.

Ho provato finestre di dimensioni diverse (1200, 3600, 7200, 36000)
36000 sono troppo grandi forse
Ho provato diversi numeri di instanze (3,6,10)
Ho diminuito l'input throughput da 200 records/sec a 100 records/sec

Tempo totale esperimento
Latenza nuova: ultimo topK in output - primo record finestra